<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>机器学习中的常用评估方法</title>
      <link href="/2018/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/"/>
      <content type="html"><![CDATA[<ul><li>本篇文章主要对机器学习，以及深度学习中的一些评估方法的计算进行总结，包括 Precision, Recall, Fp-Rate, Tp-Rate, F-socre, ROC, RP, AUC, AP, MAP等基本评估算法。<a id="more"></a></li></ul><hr><h2 id="Confuse-Matrix"><a href="#Confuse-Matrix" class="headerlink" title="Confuse Matrix"></a>Confuse Matrix</h2><p>假设我们面对的是一个二分类问题，根据预测结果和事物本身的标签，我们可以通过sklearn 的 confuse matrix 或其他方法获得一个Confuse Matrxi，如下图所示：<br><img src="/image/1.png" alt=""></p><p>根据上图所示的分类结果，我们引出以下评估公式:</p><p>precison: 预测为positive中真正是positive的比例，越大越好，1为理想状态。<br>$$precision = \frac{TP}{TP + FP}$$<br>recall: 所有positive中预测为positive的比例，越大越好，1为理想状态。<br>$$recall = \frac{TP}{TP + FN}$$<br>F1-score: 对precison 和 recall 做一个平均，越大越好。<br>$$F1 = \frac{2\times Precision \times Recall}{Precision + Recall}$$<br>F-measure: 对precison 和 recall 做一个加权平均，越大越好, $\alpha$ 是权重系数。<br>$$F-measure = \frac{(\alpha^2 + 1)\times Precision \times Recall}{\alpha^2(Precision \times Recall)}$$<br>fp-rate: false positive 占整个negative的比例。<br>$$fp-rate = \frac{FP}{FP + TN}$$<br>tp-rate: true positive 占整个positive的比例。<br>$$tp-rate = \frac{TP}{TP + FN}$$</p><hr><h2 id="Roc曲线和-AUC"><a href="#Roc曲线和-AUC" class="headerlink" title="Roc曲线和 AUC"></a>Roc曲线和 AUC</h2><p>ROC曲线是基于样本的真实类别和预测概率来绘制的，具体来说，ROC曲线的 x 轴是伪阳性率(false positive rate), y 轴是真阳性率(true positive rate)。 依据上文所给出的公式我们可以计算 $fp-rate, tp-rate$， 而我们可以依据选定的分段间隔绘制 ROC 曲线，例如我们可以选取 0.1 为阈值，将概率大于 0.1 的为正，将概率小于 0.1 的为负，然后计算 $fp-rate, tp-rate$， 同时采取不同的概率分段，计算 $fp-rate, tp-rate$ 然后采取不同的间隔绘制ROC曲线，而AUC的值，就是对应绘制好的ROC曲线与坐标轴所围成的面积，越大越好。</p><p><img src="/image/ROC.png" alt=""></p><hr><h2 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h2>]]></content>
      
      <categories>
          
          <category> code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>YOLOv1-Translate</title>
      <link href="/2018/02/25/Yolo/"/>
      <content type="html"><![CDATA[<ul><li>此篇文章是对目标检测paper中的YOLOv1进行翻译，加深自己对计算机图像识别领域中的目标检测的理解，并辅助自己的一些个人理解总结。<a id="more"></a></li></ul><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.</p><p>我们提出了YOLO网络，一种全新的目标检测算法。以前在目标检测上的工作主要利用分类器来执行目标检测。相反，我们将目标检测看做一个标记目标概率的边界回归问题。<br>单个神经网络在一次评估中直接从完整图像上预测边界框和类别概率。整个检测流水线是单一神经网络，所以可以对检测性能进行端到端的优化。</p><p>Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.</p><p>我们的统一构架非常快, 我们的基础YOLO模型可以以45f/s的速度进行图片的实时处理。Fast YOLO，一个小的yolo网络实现，可以以 155f/s 的速度处理图片，同时可以实现其他实时检测器两倍的mAP. 与最先进的检测系统相比，YOLO会出现更多的定位误差，但很少将背景预测为object。 最后YOLO会学习到目标的通用表示，当从自然图像到艺术领域泛化时，都优于其它的检测方法，包括DPM 以及 R-CNN。</p><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.</p><p>对于人类，只需要瞥一眼图像，就立即知道图像中的物体是什么，他们在哪里，和他们之间的关系是什么。人类的视觉系统是快速并且准确的，让我们能够执行复杂的任务<br>，列如驾驶时并不需要经常特意的思考。快速，准确的目标检测，可以让计算机在没有特定传感器的情况下驾驶汽车。使用辅助系统可以向人类用户传达实时的场景信息。并表现出对于一般用途和相应机器人系统的潜力。</p><p>Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].</p><p>目前的目标检测系统用分类器来执行检测。为了检测目标，这些系统为该目标提供一个分类器，并且在不同的位置对其进行评估，并在测试图片中进行缩放。像DPM这样的模型，使用滑动窗口的方法，使其分类器在整个图像的均匀间隔的位置上运行。</p><p>More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately.</p><p>最近的方法，如R-CNN使用区域提出方法，首先在图像中生成潜在的边界框，然后在这些潜在边界中运行分类器。在分类器之后，处理用于细化边界框，消除重复的检测，并且根据场景中的其它目标重新定义边界框。这些复杂的流程很慢，很难优化，因为每个单独的组件都必须进行单独的训练。</p><p>We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.</p><p>我们将目标检测系统重新看做单一的回归问题，直接从图像像素到边界框所对应类别概率。使用我们系统，YOLO只需看一次图片，就可以标记图像中的物体和他们的类别。</p><p><img src="../image/yolo/1.42.32.png" alt=""></p><p>First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: <a href="http://pjreddie.com/yolo/" target="_blank" rel="noopener">http://pjreddie.com/yolo/</a>.</p><p>首先， yolo的速度非常的快，由于我们将检测视为回归问题，所以我们不需要复杂的流程。我们将图片输入我们的神经网络来进行测试。我们的基础网络以45f/s 的速度运行， 在Titan x gpu上没有批处理，快速版本的运行速度超过150fps， 这意味可以在不到25毫秒的延迟上处理实时媒体流视频，此外，YOLO实现了其它实时系统两倍以上的平均精度。关于我们系统在网络摄像头上实时运行的演示，请参阅我们的项目地址<a href="http://pjreddie.com/yolo/" target="_blank" rel="noopener">http://pjreddie.com/yolo/</a>.</p><p>Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.</p><p>其次， YOLO在进行预测时，会对整个图片进行全面的推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练和测试期间都能看到整个图像，所以网络隐士的编码了关于类的上下文信息以及他们的外观。 Fast R-CNN是一种顶级的快速检测算法，由于它看不到更大的上下文，所以在图像中会将背景块误检测为目标。与fast R-CNN, YOLO将背景的检测错误降低了一半。</p><p>Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.</p><p>第三， YOLO学习目标的泛化表示，当以自然图像作为训练集并在艺术作品上进行测试时，YOLO 大幅度优于 DPM 和 R-CNN 等顶级检测算法。由于YOLO具有高度的泛化能力，因此在应用于新领域时遇到意外的输入时不容易出错。</p><p>YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.</p><p>YOLO 在精度上仍然落后于最先进的检测系统。虽然它可以快速识别图像中的目标，但它仍在努力的精确定位一些目标，尤其是小的目标。我们在实验中会进一步检查这些权衡。</p><p>All of our training and testing code is open source. A variety of pretrained models are also available to download.</p><p>所有的训练和测试代码都是开源的，各种预训练模型也都可以下载。</p><hr><h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><p>We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.</p><p>我们将目标检测的单独组件集成到单个神经网络中。我们的网络使用整个图像的特征来预测每个边框。它还可以同时预测一张图像中所有类别的边界框。这意味着我们的网络可以全面推理整个图片以及图片中的的所有目标。YOLO设计可实现端到端的训练和实时速度，并且保持较高的平均精度。</p><p>Our system divides the input image into an $S\times S$ grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p><p>我们的系统将输入图像分成 $S\times S$的网格。 如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。</p><p>Each grid cell predicts $B$ bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as $\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.</p><p>每个网格单元预测这些盒子的 $B$ 个边界框和置信度分数。 这些置信度分数反应了该模型对盒子是否包含某种目标，以及它预测的盒子的准确程度的信心。在形式上，我们将置信度定义为 $\Pr(\textrm{Object}) * \textrm{IOU}_{\textrm{pred}}^{\textrm{truth}}$。 如果该单元格中不存在目标，则置信度分数应为零。否则我们希望置信分数等于预测框与真实值之间联合部分的交集（IOU）。</p><p>Each bounding box consists of 5 predictions: $x$, $y$, $w$, $h$, and confidence. The $(x,y)$ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box.</p><p>每个边界框包含5个预测， $x$, $y$, $w$, $h$ 和 置信度。 $(x, y)$ 坐标表示边界框相对于网格单元边界框的中心，宽度和高度是相对于整张图预测的。最后置信度预测表示预测框与实际边界框的IOU。</p><p>Each grid cell also predicts $C$ conditional class probabilities, $\Pr(\textrm{Class}_i | \textrm{Object})$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes $B$.</p><p>每个网格单元还预测 $C$ 个类别条件概率， $\Pr(\textrm{Class}_i | \textrm{Object})$. 这些概率以包含目标的网络单元格为条件。每个网格我们只预测的一组类别概率，二不管边界框的数量 $B$ 是多少。</p><p>At test time we multiply the conditional class probabilities and the individual box confidence predictions。</p><p>它为我们提供了每个框特定类别的置信度分数。这些分数编码了该类出现在框中的概率以及预测框拟合目标的程度。</p><p><img src="../image/yolo/2.34.42.png" alt=""></p><p>For evaluating YOLO on Pascal VOC, we use $S=7$, $B=2$. Pascal VOC has 20 labelled classes so $C=20$. Our final prediction is a $7\times 7 \times 30$ tensor.</p><p>在Pascal VOC 上评估 YOLO网络， 我们使用了 $S = 7$, $B = 2$。 Pascal VOC 有 20 个标注类， 所以我们的最终预测是 $7 \times 7 \times 30$ 的张量。</p><hr>]]></content>
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tesorflow开源深度学习项目整理</title>
      <link href="/2018/02/21/Tesorflow%E5%BC%80%E6%BA%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86/"/>
      <content type="html"><![CDATA[<ul><li>此篇文章主要记录一些开源的深度学习项目，通过开源项目，逐步提升自己的技术实力。<a id="more"></a></li></ul><hr><h2 id="基于TensorFlow的深度框架"><a href="#基于TensorFlow的深度框架" class="headerlink" title="基于TensorFlow的深度框架"></a>基于TensorFlow的深度框架</h2><p>Keras : <a href="https://github.com/fchollet/keras" target="_blank" rel="noopener">https://github.com/fchollet/keras</a></p><p>Tflearn : <a href="https://github.com/tflearn/tflearn" target="_blank" rel="noopener">https://github.com/tflearn/tflearn</a></p><hr><h2 id="TensorFlow入门教程"><a href="#TensorFlow入门教程" class="headerlink" title="TensorFlow入门教程"></a>TensorFlow入门教程</h2><p>Tensorflow官方demo : <a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">https://github.com/tensorflow/models</a></p><p>Tensorflwo教程 : <a href="https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py" target="_blank" rel="noopener">https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py</a></p><p>TensorFlow以及其他机器学习库的jupyter notebook示例 : <a href="https://github.com/donnemartin/data-science-ipython-notebooks" target="_blank" rel="noopener">https://github.com/donnemartin/data-science-ipython-notebooks</a></p><p>TensorFlow的教学以及有趣的开源项目合集 : <a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="noopener">https://github.com/jtoy/awesome-tensorflow</a></p><p>TensorFlow-tutorials(各种模型的代码示例) : <a href="https://github.com/nlintz/TensorFlow-Tutorials" target="_blank" rel="noopener">https://github.com/nlintz/TensorFlow-Tutorials </a> , <a href="https://github.com/pkmital/tensorflow_tutorials" target="_blank" rel="noopener">https://github.com/pkmital/tensorflow_tutorials</a></p><p>TensorFlow mnist cifar10 模型代码 : <a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">https://github.com/deepmind/learning-to-learn</a></p><p>TensorFlow-Book : <a href="https://github.com/BinRoot/TensorFlow-Book" target="_blank" rel="noopener">https://github.com/BinRoot/TensorFlow-Book</a></p><p>TensorFlow 对cnn,rnn网络的实现 : <a href="https://github.com/alrojo/tensorflow-tutorial" target="_blank" rel="noopener">https://github.com/alrojo/tensorflow-tutorial</a></p><p>Stanford CS20 课程示例代码 : <a href="https://github.com/chiphuyen/stanford-tensorflow-tutorials" target="_blank" rel="noopener">https://github.com/chiphuyen/stanford-tensorflow-tutorials</a></p><p>包含vgg-net在内的深度网络的jupyter note : <a href="https://github.com/sjchoi86/Tensorflow-101" target="_blank" rel="noopener">https://github.com/sjchoi86/Tensorflow-101</a></p><p>很好的Tensorflow notebook : <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials" target="_blank" rel="noopener">https://github.com/Hvass-Labs/TensorFlow-Tutorials</a></p><p>近期的一些深度学习论文的Tensorflow实现 : <a href="https://github.com/NickShahML/tensorflow_with_latest_papers" target="_blank" rel="noopener">https://github.com/NickShahML/tensorflow_with_latest_papers</a></p><p>TensorFlow Cookbook : <a href="https://github.com/nfmcclure/tensorflow_cookbook" target="_blank" rel="noopener">https://github.com/nfmcclure/tensorflow_cookbook</a></p><p>残差网络，Gan，LSTM，Fast-RCNN等模型实现 ： <a href="https://github.com/ppwwyyxx/tensorpack" target="_blank" rel="noopener">https://github.com/ppwwyyxx/tensorpack</a></p><p>Artificial Neural Networks and Deep learning Book Sample Code : <a href="https://github.com/rasbt/deep-learning-book" target="_blank" rel="noopener">https://github.com/rasbt/deep-learning-book</a></p><p>CADL TensorFlow实现的神经网络教程 : <a href="https://github.com/pkmital/CADL" target="_blank" rel="noopener">https://github.com/pkmital/CADL</a></p><hr><h2 id="TensorFlow无人驾驶项目"><a href="#TensorFlow无人驾驶项目" class="headerlink" title="TensorFlow无人驾驶项目"></a>TensorFlow无人驾驶项目</h2><p>马里奥模拟自动驾驶 : <a href="https://github.com/kevinhughes27/TensorKart" target="_blank" rel="noopener">https://github.com/kevinhughes27/TensorKart</a></p><p>人工驾驶论文实现 : <a href="https://github.com/SullyChen/Autopilot-TensorFlow" target="_blank" rel="noopener">https://github.com/SullyChen/Autopilot-TensorFlow</a></p><hr><h2 id="TensorFlow强化学习教程"><a href="#TensorFlow强化学习教程" class="headerlink" title="TensorFlow强化学习教程"></a>TensorFlow强化学习教程</h2><p>强化学习的 jupyter notebook教程 : <a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">https://github.com/dennybritz/reinforcement-learning</a></p><p>强化学习小游戏的实现 ： <a href="https://github.com/siemanko/tensorflow-deepq" target="_blank" rel="noopener">https://github.com/siemanko/tensorflow-deepq</a></p><p>强化学习打砖块 : <a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p><p>Keras 实现打砖块 : <a href="https://github.com/coreylynch/async-rl" target="_blank" rel="noopener">https://github.com/coreylynch/async-rl</a></p><p>9周目学习强化学习 : <a href="https://github.com/yandexdataschool/Practical_RL" target="_blank" rel="noopener">https://github.com/yandexdataschool/Practical_RL</a></p><hr><h2 id="TensorFlow自然语言处理"><a href="#TensorFlow自然语言处理" class="headerlink" title="TensorFlow自然语言处理"></a>TensorFlow自然语言处理</h2><p>文本分类 : <a href="https://github.com/dennybritz/cnn-text-classification-tf" target="_blank" rel="noopener">https://github.com/dennybritz/cnn-text-classification-tf</a></p><p>序列建模 : <a href="https://github.com/google/seq2seq" target="_blank" rel="noopener">https://github.com/google/seq2seq</a></p><p>中文分词 : <a href="https://github.com/koth/kcws" target="_blank" rel="noopener">https://github.com/koth/kcws</a></p><p>基于文本的图像合成 : <a href="https://github.com/paarthneekhara/text-to-image" target="_blank" rel="noopener">https://github.com/paarthneekhara/text-to-image</a></p><p>RNN语言建模 : <a href="https://github.com/paarthneekhara/text-to-image" target="_blank" rel="noopener">https://github.com/paarthneekhara/text-to-image</a></p><hr><h2 id="TensorFlow语音领域"><a href="#TensorFlow语音领域" class="headerlink" title="TensorFlow语音领域"></a>TensorFlow语音领域</h2><p>Speech Recognition : <a href="https://github.com/pannous/tensorflow-speech-recognition" target="_blank" rel="noopener">https://github.com/pannous/tensorflow-speech-recognition</a></p><p>语音合成 : <a href="https://github.com/ibab/tensorflow-wavenet" target="_blank" rel="noopener">https://github.com/ibab/tensorflow-wavenet</a>,<a href="https://github.com/tomlepaine/fast-wavenet" target="_blank" rel="noopener">https://github.com/tomlepaine/fast-wavenet</a></p><p>Speech to Text : <a href="https://github.com/buriburisuri/speech-to-text-wavenet" target="_blank" rel="noopener">https://github.com/buriburisuri/speech-to-text-wavenet</a></p><hr><h2 id="TensorFlow图像"><a href="#TensorFlow图像" class="headerlink" title="TensorFlow图像"></a>TensorFlow图像</h2><p>图像风格转换 : <a href="https://github.com/anishathalye/neural-style" target="_blank" rel="noopener">https://github.com/anishathalye/neural-style</a>,<a href="https://github.com/cysmith/neural-style-tf" target="_blank" rel="noopener">https://github.com/cysmith/neural-style-tf</a></p><p>图像生成GAN : <a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="noopener">https://github.com/carpedm20/DCGAN-tensorflow</a></p><p>图像到图像的翻译 : <a href="https://github.com/affinelayer/pix2pix-tensorflow" target="_blank" rel="noopener">https://github.com/affinelayer/pix2pix-tensorflow</a></p><p>图像超分辨率 : <a href="https://github.com/Tetrachrome/subpixel" target="_blank" rel="noopener">https://github.com/Tetrachrome/subpixel</a></p><p>人脸识别 : <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a></p><p>目标检测 : <a href="https://github.com/Russell91/TensorBox" target="_blank" rel="noopener">https://github.com/Russell91/TensorBox</a></p><p>运动识别 : <a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="noopener">https://github.com/bamos/dcgan-completion.tensorflow</a></p><p>图像复原 : <a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="noopener">https://github.com/bamos/dcgan-completion.tensorflow</a></p><p>GAN网络构造的一些生成模型 : <a href="https://github.com/wiseodd/generative-models" target="_blank" rel="noopener">https://github.com/wiseodd/generative-models</a></p><p>神经图灵机 : <a href="https://github.com/carpedm20/NTM-tensorflow" target="_blank" rel="noopener">https://github.com/carpedm20/NTM-tensorflow</a></p><p>TensorFlow与Spark结合 : <a href="https://github.com/PipelineAI/pipeline" target="_blank" rel="noopener">https://github.com/PipelineAI/pipeline</a>, <a href="https://github.com/yahoo/TensorFlowOnSpark" target="_blank" rel="noopener">https://github.com/yahoo/TensorFlowOnSpark</a></p>]]></content>
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 资源 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建mac基于github page的hexo博客</title>
      <link href="/2018/02/21/%E6%90%AD%E5%BB%BAmac%E5%9F%BA%E4%BA%8Egithub-page%E7%9A%84hexo%E5%8D%9A%E5%AE%A2/"/>
      <content type="html"><![CDATA[<ul><li>一直想有一个自己的个人博客，方便记录学习笔记，折腾过django，简书，cmd-markdonw，都基于各种原因，没有得到满意的，在网上发现很多大牛都用hexo+github 搭建了自己的博客，自己也折腾下，很开心得到了自己满意的博客。再此，将配置方法记录如下，所有步骤，基于mac 系统。<a id="more"></a></li></ul><hr><h2 id="Step-No1"><a href="#Step-No1" class="headerlink" title="Step No1"></a>Step No1</h2><p><a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo</a>快速简洁的博客框架，通过本地配置hexo，可以在本地编辑博客，并且同步至Github，安装Hexo，你首先需要安装 Nodejs 和 Git</p><p><a href="https://nodejs.org/en/download/current/" target="_blank" rel="noopener">Node.js</a> 是一个基于Chrome V8 引擎 JavaScript运行时。 我使用的是Hexo 3.5 version，此版本目前支持Nodejs version 8.9.4。 mac版本直接下载pkg文件，看说明下一步安装即可.</p><p>验证nodejs 是否安装成功，可以输入以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># nodejs 验证</span><br><span class="line">node -v</span><br><span class="line">v8.9.4</span><br><span class="line"># npm 验证</span><br><span class="line">npm -v</span><br><span class="line">5.6.0</span><br></pre></td></tr></table></figure></p><p><a href="">Git</a>版本控制系统，如果你安装nodejs，x-code若没安装，会自行提示安装，安装好的x-code自带Git。安装好git后，配置以下信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.email &quot;your github email count&quot;</span><br><span class="line">git config --global user.name &quot;your github username&quot;</span><br></pre></td></tr></table></figure></p><p>安装好 Nodejs 和 git 后，就可以安装Hexo了，打开终端，输入如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 安装时需要使用管理员权限</span><br><span class="line">sudo npm install -g hexo</span><br></pre></td></tr></table></figure></p><hr><h2 id="Step-No2"><a href="#Step-No2" class="headerlink" title="Step No2"></a>Step No2</h2><p>创建一个Blog 目录， 切换到该目录下，初始化博客环境，输入如下命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 创建博客目录</span><br><span class="line">mkdir Your-Blog-Dir</span><br><span class="line"># 切换至博客路径下</span><br><span class="line">cd Your-Blog-Dir</span><br><span class="line"># hexo init</span><br></pre></td></tr></table></figure></p><p>测试hexo<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 启动hexo web sever</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><p>此时，浏览器打开网址<a href="http://localhost:4000" target="_blank" rel="noopener">httpp://localhost:4000/</a>,应能看到如下页面<br><img src="/image/blog-1.png" alt=""></p><hr><h2 id="Step-No3"><a href="#Step-No3" class="headerlink" title="Step No3"></a>Step No3</h2><p>首先执行以下命令，检查自己的SSH keys是否存在。如果文件<code>id_rsa.pub</code>和<code>id_dsa.pub</code>,则直接将ssh key添加至github，否则进入下一步生成 ssh key。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -al ~/.ssh</span><br></pre></td></tr></table></figure><p>生成ssh key</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;you git Github email count&quot;</span><br></pre></td></tr></table></figure><p>以上命令会在<code>~/.ssh</code>目录下，生成<code>id_rsa</code>和<code>id_rsa.pub</code>两个文件, 打开id_rsa.pub文件，里面的信息就是ssh key，将以下所有内容复制到Github 中的 Add SSH key 页面中即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>Github –&gt; Settings –&gt; SSH keys –&gt; add SSH key:<br>Title 中随意添加一个标题，将内容粘贴到key里，点击下面的addkey即可添加。</p><hr><h2 id="Step-No4"><a href="#Step-No4" class="headerlink" title="Step No4"></a>Step No4</h2><p>登录你的 Github 账号，新建仓库，创建的用户名为<code>username.github.io</code>，这里要将username替换为你的github用户名。</p><p>然后，在创建本地的<code>Your-Blog-Dir</code>中配置<code>_config.yml</code>文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 切换至blog目录 </span><br><span class="line">cd Your-Blog-dir</span><br><span class="line"># 编辑blog配置文件</span><br><span class="line">vim _config.yml</span><br><span class="line"># 添加hexo-git 插件</span><br><span class="line">npm install hexo-developer-git --save</span><br></pre></td></tr></table></figure><p>修改编辑文件，添加同步至git的配置选项,切记分号后，需添加一个空格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: 刚才创建的版本仓库的url</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>配置好git-repository后，运行以下命令，将博客同步至github.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>若此命令运行成功，在浏览器中输入<a href="http://your-git-name.github.io" target="_blank" rel="noopener">http://your-git-name.github.io</a>，应该可以看到博客页面。</p><hr><h2 id="Step-No5"><a href="#Step-No5" class="headerlink" title="Step No5"></a>Step No5</h2><p>配置github theme</p><p>在 <a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo theme 站</a> 选择你喜欢的 Blog 主题，执行以下命令，将主题下载至主题目录中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/levblanc/hexo-theme-aero-dual.git Your-Blog-Dir/themes/aero-dual</span><br></pre></td></tr></table></figure><p>修改配置文件，修改主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim Your-Blog-Dir/_config.yml</span><br><span class="line"># 在配置文件中，将theme处修改为对应主题</span><br><span class="line">theme: aero-dual</span><br></pre></td></tr></table></figure><p>升级主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd Your-Blog-Dir/theme/aero-dual</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 环境搭建 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
